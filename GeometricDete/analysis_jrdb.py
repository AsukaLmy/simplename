#!/usr/bin/env python3
"""
JRDB Dataset Label Co-occurrence Analysis
Analyzes label interaction patterns and validates multi-level classification design
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
from pathlib import Path
import argparse
from typing import Dict, List, Tuple, Set
import warnings
warnings.filterwarnings('ignore')

# Try to import chord visualization
try:
    from chord import Chord
    CHORD_AVAILABLE = True
except ImportError:
    print("Warning: chord package not available. Install with: pip install chord")
    CHORD_AVAILABLE = False

# For network visualization
try:
    import networkx as nx
    import plotly.graph_objects as go
    import plotly.express as px
    NETWORK_AVAILABLE = True
except ImportError:
    print("Warning: networkx/plotly not available. Some visualizations will be skipped")
    NETWORK_AVAILABLE = False


class JRDBLabelAnalyzer:
    """JRDBæ•°æ®é›†æ ‡ç­¾å…±ç°åˆ†æå™¨"""
    
    def __init__(self, data_path: str):
        """
        Args:
            data_path: JRDBæ•°æ®é›†è·¯å¾„ï¼ŒåŒ…å«labels/labels_2d_activity_social_stitched/
        """
        self.data_path = Path(data_path)
        self.social_labels_path = self.data_path / "labels" / "labels_2d_activity_social_stitched"
        
        # ä¸»è¦å…³æ³¨çš„ä¸‰ä¸ªç±»åˆ«
        self.main_categories = {
            'walking_together': 'walking together',
            'standing_together': 'standing together', 
            'sitting_together': 'sitting together'
        }
        
        # æ‰€æœ‰äº¤äº’æ ‡ç­¾
        self.all_interactions = set()
        
        # æ•°æ®å­˜å‚¨
        self.interaction_pairs = []  # List[Dict]: æ¯ä¸ªäº¤äº’å¯¹çš„ä¿¡æ¯
        self.label_counts = Counter()  # æ ‡ç­¾é¢‘æ¬¡ç»Ÿè®¡
        self.co_occurrence_matrix = defaultdict(lambda: defaultdict(int))  # å…±ç°çŸ©é˜µ
        self.scene_label_stats = defaultdict(lambda: defaultdict(int))  # åœºæ™¯-æ ‡ç­¾ç»Ÿè®¡
        
        print(f"JRDB Label Analyzer initialized")
        print(f"Data path: {self.data_path}")
        print(f"Social labels path: {self.social_labels_path}")
    
    def load_and_process_data(self):
        """åŠ è½½å¹¶å¤„ç†æ‰€æœ‰JRDBç¤¾äº¤æ ‡æ³¨æ•°æ®"""
        print(f"\nğŸ”„ Loading JRDB social activity labels...")
        
        if not self.social_labels_path.exists():
            raise FileNotFoundError(f"Social labels path not found: {self.social_labels_path}")
        
        json_files = list(self.social_labels_path.glob("*.json"))
        print(f"Found {len(json_files)} scene files")
        
        total_interactions = 0
        
        for json_file in json_files:  # å¤„ç†æ‰€æœ‰æ–‡ä»¶
            scene_name = json_file.stem
            print(f"Processing scene: {scene_name}")
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    scene_data = json.load(f)
                
                scene_interactions = self._process_scene_data(scene_data, scene_name)
                total_interactions += scene_interactions
                if scene_interactions > 0:
                    print(f"  Found {scene_interactions} interactions in {scene_name}")
                else:
                    print(f"  No interactions found in {scene_name}")
                
            except Exception as e:
                print(f"  âŒ Error processing {scene_name}: {e}")
                continue
        
        print(f"âœ… Data loading completed:")
        print(f"  Total scenes: {len(json_files)}")
        print(f"  Total interactions: {total_interactions}")
        print(f"  Unique interaction types: {len(self.all_interactions)}")
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰¾åˆ°çš„æ ‡ç­¾å’Œè®¡æ•°
        print(f"\nTop 10 interaction labels found:")
        for label, count in self.label_counts.most_common(10):
            print(f"  {label}: {count}")
            
        # è°ƒè¯•ï¼šæ˜¾ç¤ºå…±ç°æƒ…å†µ
        if self.co_occurrence_matrix:
            print(f"\nSample co-occurrences:")
            sample_count = 0
            for label1, co_labels in self.co_occurrence_matrix.items():
                if co_labels and sample_count < 3:
                    print(f"  {label1} co-occurs with:")
                    for label2, count in list(co_labels.items())[:3]:
                        print(f"    {label2}: {count}")
                    sample_count += 1
        
        return total_interactions
    
    def _process_scene_data(self, scene_data: dict, scene_name: str) -> int:
        """å¤„ç†å•ä¸ªåœºæ™¯çš„æ•°æ®"""
        interactions_count = 0
        
        labels = scene_data.get('labels', {})
        
        for frame_name, frame_data in labels.items():
            # æŒ‰äººç‰©å¯¹èšåˆäº¤äº’æ ‡ç­¾
            person_pair_interactions = defaultdict(list)
            
            for person_data in frame_data:
                h_interactions = person_data.get('H-interaction', [])
                person_id = person_data.get('label_id', '')
                
                for interaction in h_interactions:
                    # æå–äº¤äº’æ ‡ç­¾
                    inter_labels = interaction.get('inter_labels', {})
                    active_labels = [label for label, value in inter_labels.items() if value > 0]
                    
                    if not active_labels:
                        continue
                    
                    pair_id = interaction.get('pair', '')
                    
                    # åˆ›å»ºç»Ÿä¸€çš„äººç‰©å¯¹key (ä¿è¯é¡ºåºä¸€è‡´)
                    pair_key = tuple(sorted([person_id, pair_id]))
                    
                    # å°†æ ‡ç­¾æ·»åŠ åˆ°è¿™ä¸ªäººç‰©å¯¹
                    person_pair_interactions[pair_key].extend(active_labels)
            
            # å¤„ç†æ¯ä¸ªäººç‰©å¯¹çš„èšåˆæ ‡ç­¾
            for pair_key, all_labels in person_pair_interactions.items():
                # å»é‡å¹¶æ’åº
                unique_labels = list(set(all_labels))
                
                # è®°å½•äº¤äº’å¯¹ä¿¡æ¯
                interaction_info = {
                    'scene': scene_name,
                    'frame': frame_name,
                    'person_pair': pair_key,
                    'labels': unique_labels,
                    'label_count': len(unique_labels)
                }
                
                self.interaction_pairs.append(interaction_info)
                interactions_count += 1
                
                # è°ƒè¯•ï¼šæ˜¾ç¤ºå¤šæ ‡ç­¾æƒ…å†µ
                if len(unique_labels) > 1 and interactions_count <= 10:
                    print(f"    Multi-label pair {pair_key}: {unique_labels}")
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._update_statistics(unique_labels, scene_name)
        
        return interactions_count
    
    def _update_statistics(self, labels: List[str], scene_name: str):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        # æ ‡ç­¾é¢‘æ¬¡
        for label in labels:
            self.label_counts[label] += 1
            self.all_interactions.add(label)
            self.scene_label_stats[scene_name][label] += 1
        
        # æ ‡ç­¾å…±ç°çŸ©é˜µ - å¯¹äºå¤šæ ‡ç­¾æƒ…å†µï¼Œæ¯ä¸ªæ ‡ç­¾å¯¹è®¡æ•°ä¸€æ¬¡
        if len(labels) > 1:  # åªæœ‰å¤šæ ‡ç­¾æƒ…å†µæ‰æœ‰å…±ç°
            for i, label1 in enumerate(labels):
                for j, label2 in enumerate(labels):
                    if i != j:  # ä¸åŒæ ‡ç­¾é—´çš„å…±ç°
                        self.co_occurrence_matrix[label1][label2] += 1
    
    def calculate_key_metrics(self) -> Dict:
        """è®¡ç®—ä¸»è¦ä¸‰ç±»çš„å…³é”®æŒ‡æ ‡"""
        print(f"\nğŸ“Š Calculating key metrics for main categories...")
        
        metrics = {}
        total_interactions = sum(self.label_counts.values())
        
        for category_key, category_label in self.main_categories.items():
            if category_label not in self.label_counts:
                print(f"  âš ï¸ Warning: {category_label} not found in data")
                continue
            
            count = self.label_counts[category_label]
            percentage = (count / total_interactions) * 100
            
            # è®¡ç®—è¯¥æ ‡ç­¾çš„å…±ç°æƒ…å†µ
            co_labels = self.co_occurrence_matrix[category_label]
            total_co_occurrences = sum(co_labels.values())
            
            # å•ç‹¬å‡ºç° vs ä¸å…¶ä»–æ ‡ç­¾å…±ç°
            solo_appearances = count - total_co_occurrences
            co_occurrence_rate = (total_co_occurrences / count) * 100 if count > 0 else 0
            
            # æœ€å¸¸å…±ç°çš„æ ‡ç­¾
            top_co_labels = dict(Counter(co_labels).most_common(5))
            
            metrics[category_key] = {
                'label': category_label,
                'count': count,
                'percentage': percentage,
                'solo_appearances': solo_appearances,
                'co_occurrence_rate': co_occurrence_rate,
                'top_co_labels': top_co_labels,
                'total_co_occurrences': total_co_occurrences
            }
            
            print(f"  {category_label}:")
            print(f"    Count: {count:,} ({percentage:.2f}%)")
            print(f"    Co-occurrence rate: {co_occurrence_rate:.2f}%")
            print(f"    Solo appearances: {solo_appearances:,}")
        
        # ä¸‰ç±»æ€»è¦†ç›–ç‡
        main_three_total = sum(self.label_counts[label] for label in self.main_categories.values() 
                              if label in self.label_counts)
        main_coverage = (main_three_total / total_interactions) * 100
        
        metrics['overall'] = {
            'total_interactions': total_interactions,
            'main_three_coverage': main_coverage,
            'unique_labels': len(self.all_interactions)
        }
        
        print(f"\nğŸ“ˆ Overall Statistics:")
        print(f"  Main three categories coverage: {main_coverage:.2f}%")
        print(f"  Total unique interaction types: {len(self.all_interactions)}")
        
        # è®¡ç®—æ ‡ç­¾åœ¨interactionä¸­çš„å‡ºç°æ¯”ä¾‹
        interaction_label_stats = self._calculate_interaction_label_proportion()
        metrics['interaction_label_stats'] = interaction_label_stats
        
        return metrics
    
    def create_co_occurrence_matrix(self, top_n: int = 15) -> pd.DataFrame:
        """åˆ›å»ºæ ‡ç­¾å…±ç°çŸ©é˜µ"""
        print(f"\nCreating co-occurrence matrix (top {top_n} labels)...")
        
        # é€‰æ‹©æœ€é¢‘ç¹çš„æ ‡ç­¾
        top_labels = [label for label, _ in self.label_counts.most_common(top_n)]
        print(f"Top labels selected: {top_labels[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ª
        
        # åˆ›å»ºå…±ç°çŸ©é˜µ
        matrix = np.zeros((len(top_labels), len(top_labels)))
        
        for i, label1 in enumerate(top_labels):
            for j, label2 in enumerate(top_labels):
                if i != j:
                    # è®¡ç®—æ¡ä»¶æ¦‚ç‡ P(label2|label1)
                    co_count = self.co_occurrence_matrix[label1][label2]
                    total_label1 = self.label_counts[label1]
                    if total_label1 > 0:
                        matrix[i][j] = (co_count / total_label1) * 100
                    
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºä¸€äº›å…±ç°æ•°æ®
                    if i < 3 and j < 3 and co_count > 0:
                        print(f"  {label1} -> {label2}: {co_count}/{total_label1} = {matrix[i][j]:.1f}%")
        
        df_matrix = pd.DataFrame(matrix, index=top_labels, columns=top_labels)
        
        # æ£€æŸ¥çŸ©é˜µæ˜¯å¦å…¨é›¶
        max_value = df_matrix.values.max()
        print(f"Matrix max value: {max_value:.2f}%")
        if max_value == 0:
            print("Warning: Co-occurrence matrix is all zeros!")
        
        return df_matrix
    
    def plot_heatmap(self, matrix: pd.DataFrame, save_path: str = None):
        """ç»˜åˆ¶æ ‡ç­¾å…±ç°çƒ­åŠ›å›¾"""
        print(f"\nğŸ¨ Creating co-occurrence heatmap...")
        
        plt.figure(figsize=(14, 12))
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        mask = np.diag(np.ones(len(matrix)))  # éšè—å¯¹è§’çº¿
        
        ax = sns.heatmap(
            matrix, 
            annot=True, 
            fmt='.1f',
            cmap='YlOrRd',
            mask=mask,
            square=True,
            cbar_kws={'label': 'Co-occurrence Rate (%)'},
            annot_kws={'size': 8}
        )
        
        plt.title('JRDB H-Interaction Co-occurrence Matrix\n(Conditional Probability: P(Column|Row))', 
                 fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('Co-occurring Label', fontsize=12, fontweight='bold')
        plt.ylabel('Primary Label', fontsize=12, fontweight='bold')
        
        # æ—‹è½¬æ ‡ç­¾ä»¥æé«˜å¯è¯»æ€§
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        # çªå‡ºæ˜¾ç¤ºä¸»è¦ä¸‰ç±»
        for i, label in enumerate(matrix.index):
            if label in self.main_categories.values():
                ax.add_patch(plt.Rectangle((0, i), len(matrix.columns), 1, 
                                         fill=False, edgecolor='blue', lw=3))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"  ğŸ’¾ Heatmap saved to: {save_path}")
        
        plt.show()
    
    def create_chord_diagram(self, top_n: int = 10, save_path: str = None):
        """åˆ›å»ºå’Œå¼¦å›¾"""
        if not CHORD_AVAILABLE:
            print("âŒ Chord diagram requires 'chord' package. Skipping...")
            return
        
        print(f"\nğŸµ Creating chord diagram (top {top_n} labels)...")
        
        # é€‰æ‹©æœ€é¢‘ç¹çš„æ ‡ç­¾
        top_labels = [label for label, _ in self.label_counts.most_common(top_n)]
        
        # åˆ›å»ºå…±ç°çŸ©é˜µæ•°æ®
        matrix_data = []
        for label1 in top_labels:
            row = []
            for label2 in top_labels:
                if label1 == label2:
                    row.append(0)
                else:
                    # ä½¿ç”¨å¯¹ç§°çš„å…±ç°è®¡æ•°
                    co_count = (self.co_occurrence_matrix[label1][label2] + 
                               self.co_occurrence_matrix[label2][label1]) / 2
                    row.append(co_count)
            matrix_data.append(row)
        
        # åˆ›å»ºå’Œå¼¦å›¾
        try:
            # ç®€åŒ–æ ‡ç­¾åç§°
            short_labels = [label.replace(' together', '').replace('walking', 'walk')
                           .replace('standing', 'stand').replace('sitting', 'sit')
                           .replace('conversation', 'conv') for label in top_labels]
            
            chord = Chord(matrix_data, short_labels)
            chord.to_html(save_path if save_path else "chord_diagram.html")
            print(f"  ğŸ’¾ Chord diagram saved to: {save_path or 'chord_diagram.html'}")
            
        except Exception as e:
            print(f"  âŒ Failed to create chord diagram: {e}")
    
    def analyze_scene_patterns(self) -> Dict:
        """åˆ†æä¸åŒåœºæ™¯çš„æ ‡ç­¾æ¨¡å¼"""
        print(f"\nğŸ¢ Analyzing scene-specific patterns...")
        
        scene_analysis = {}
        
        for scene_name, label_stats in self.scene_label_stats.items():
            total_scene_interactions = sum(label_stats.values())
            
            # è®¡ç®—è¯¥åœºæ™¯ä¸­ä¸»è¦ä¸‰ç±»çš„åˆ†å¸ƒ
            main_distribution = {}
            for category_key, category_label in self.main_categories.items():
                count = label_stats.get(category_label, 0)
                percentage = (count / total_scene_interactions) * 100 if total_scene_interactions > 0 else 0
                main_distribution[category_key] = {
                    'count': count,
                    'percentage': percentage
                }
            
            # æ‰¾å‡ºè¯¥åœºæ™¯çš„ç‰¹è‰²æ ‡ç­¾
            scene_total = sum(label_stats.values())
            distinctive_labels = {}
            for label, count in label_stats.items():
                global_rate = (self.label_counts[label] / sum(self.label_counts.values())) * 100
                scene_rate = (count / scene_total) * 100
                if scene_rate > global_rate * 1.5:  # åœºæ™¯ä¸­çš„æ¯”ä¾‹æ¯”å…¨å±€é«˜50%ä»¥ä¸Š
                    distinctive_labels[label] = {
                        'scene_rate': scene_rate,
                        'global_rate': global_rate,
                        'enrichment': scene_rate / global_rate
                    }
            
            scene_analysis[scene_name] = {
                'total_interactions': total_scene_interactions,
                'main_distribution': main_distribution,
                'distinctive_labels': distinctive_labels
            }
        
        return scene_analysis
    
    def _calculate_interaction_label_proportion(self) -> Dict:
        """è®¡ç®—æ¯ä¸ªæ ‡ç­¾åœ¨æ‰€æœ‰interactionä¸­å‡ºç°çš„æ¯”ä¾‹"""
        print(f"\nğŸ“Š Calculating interaction label proportions...")
        
        # ç»Ÿè®¡åŒ…å«æ¯ä¸ªæ ‡ç­¾çš„interactionæ•°é‡
        label_interaction_counts = defaultdict(int)
        total_interactions = len(self.interaction_pairs)
        
        for interaction in self.interaction_pairs:
            labels_in_interaction = set(interaction['labels'])  # å»é‡ï¼Œä¸€ä¸ªinteractionä¸­ç›¸åŒæ ‡ç­¾åªè®¡ç®—ä¸€æ¬¡
            
            for label in labels_in_interaction:
                label_interaction_counts[label] += 1
        
        # è®¡ç®—æ¯”ä¾‹
        label_proportions = {}
        for label, interaction_count in label_interaction_counts.items():
            proportion = (interaction_count / total_interactions) * 100 if total_interactions > 0 else 0
            label_proportions[label] = {
                'interaction_count': interaction_count,
                'total_interactions': total_interactions,
                'proportion': proportion
            }
        
        # æŒ‰æ¯”ä¾‹æ’åº
        sorted_proportions = dict(sorted(label_proportions.items(), 
                                       key=lambda x: x[1]['proportion'], reverse=True))
        
        # æ‰“å°Top 10
        print(f"  Top 10 labels by interaction proportion:")
        for i, (label, stats) in enumerate(list(sorted_proportions.items())[:10]):
            print(f"    {i+1:2d}. {label}: {stats['interaction_count']:,}/{total_interactions:,} "
                  f"interactions ({stats['proportion']:.2f}%)")
        
        # ç‰¹åˆ«å…³æ³¨ä¸»è¦ä¸‰ç±»
        print(f"\n  Main three categories in interactions:")
        for category_key, category_label in self.main_categories.items():
            if category_label in sorted_proportions:
                stats = sorted_proportions[category_label]
                print(f"    {category_label}: {stats['interaction_count']:,}/{total_interactions:,} "
                      f"interactions ({stats['proportion']:.2f}%)")
            else:
                print(f"    {category_label}: 0 interactions (0.00%)")
        
        return sorted_proportions
    
    def plot_main_categories_analysis(self, metrics: Dict, save_path: str = None):
        """ç»˜åˆ¶ä¸»è¦ä¸‰ç±»çš„åˆ†æå›¾è¡¨"""
        print(f"\nğŸ“Š Creating main categories analysis plots...")
        
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        
        # 1. é¢‘ç‡åˆ†å¸ƒé¥¼å›¾
        ax1 = axes[0, 0]
        categories = list(self.main_categories.keys())
        counts = [metrics[cat]['count'] for cat in categories if cat in metrics]
        labels = [metrics[cat]['label'] for cat in categories if cat in metrics]
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        if counts:
            wedges, texts, autotexts = ax1.pie(counts, labels=labels, autopct='%1.1f%%', 
                                              colors=colors, startangle=90)
        ax1.set_title('Main Categories Distribution\n(by Label Count)', fontsize=12, fontweight='bold')
        
        # 2. å…±ç°ç‡å¯¹æ¯”
        ax2 = axes[0, 1]
        co_rates = [metrics[cat]['co_occurrence_rate'] for cat in categories if cat in metrics]
        bars = ax2.bar(range(len(co_rates)), co_rates, color=colors[:len(co_rates)])
        ax2.set_xticks(range(len(co_rates)))
        ax2.set_xticklabels([cat.replace('_', ' ').title() for cat in categories if cat in metrics], rotation=45)
        ax2.set_ylabel('Co-occurrence Rate (%)')
        ax2.set_title('Co-occurrence Rates', fontsize=12, fontweight='bold')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars, co_rates):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{rate:.1f}%', ha='center', va='bottom')
        
        # 3. Interactionæ¯”ä¾‹åˆ†æ
        ax3 = axes[0, 2]
        if 'interaction_label_stats' in metrics:
            interaction_stats = metrics['interaction_label_stats']
            interaction_props = []
            for cat in categories:
                cat_label = self.main_categories[cat]
                if cat_label in interaction_stats:
                    interaction_props.append(interaction_stats[cat_label]['proportion'])
                else:
                    interaction_props.append(0)
            
            bars3 = ax3.bar(range(len(interaction_props)), interaction_props, color=colors[:len(interaction_props)])
            ax3.set_xticks(range(len(interaction_props)))
            ax3.set_xticklabels([cat.replace('_', ' ').title() for cat in categories], rotation=45)
            ax3.set_ylabel('Interaction Proportion (%)')
            ax3.set_title('Interaction Coverage\n(% of total interactions)', fontsize=12, fontweight='bold')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, prop in zip(bars3, interaction_props):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                        f'{prop:.1f}%', ha='center', va='bottom')
        
        # 4. å•ç‹¬ vs å…±ç°å‡ºç°
        ax4 = axes[1, 0]
        solo_counts = [metrics[cat]['solo_appearances'] for cat in categories if cat in metrics]
        co_counts = [metrics[cat]['total_co_occurrences'] for cat in categories if cat in metrics]
        
        width = 0.35
        x = np.arange(len(solo_counts))
        ax4.bar(x - width/2, solo_counts, width, label='Solo', color='lightblue')
        ax4.bar(x + width/2, co_counts, width, label='Co-occurring', color='orange')
        
        ax4.set_xticks(x)
        ax4.set_xticklabels([cat.replace('_', ' ').title() for cat in categories if cat in metrics], rotation=45)
        ax4.set_ylabel('Count')
        ax4.set_title('Solo vs Co-occurring Appearances', fontsize=12, fontweight='bold')
        ax4.legend()
        
        # 5. è¦†ç›–ç‡åˆ†æ (Label vs Interaction)
        ax5 = axes[1, 1]
        if 'overall' in metrics:
            main_coverage = metrics['overall']['main_three_coverage']
            other_coverage = 100 - main_coverage
            
            coverage_data = [main_coverage, other_coverage]
            coverage_labels = ['Main 3 Categories', 'Other Interactions']
            coverage_colors = ['#2ECC71', '#E74C3C']
            
            ax5.pie(coverage_data, labels=coverage_labels, autopct='%1.1f%%', 
                   colors=coverage_colors, startangle=90)
            ax5.set_title('Label Count Coverage', fontsize=12, fontweight='bold')
        
        # 6. äº¤äº’è¦†ç›–ç‡åˆ†æ
        ax6 = axes[1, 2]
        if 'interaction_label_stats' in metrics:
            interaction_stats = metrics['interaction_label_stats']
            main_interaction_coverage = 0
            for cat in categories:
                cat_label = self.main_categories[cat]
                if cat_label in interaction_stats:
                    main_interaction_coverage += interaction_stats[cat_label]['proportion']
            
            other_interaction_coverage = 100 - main_interaction_coverage
            
            interaction_coverage_data = [main_interaction_coverage, other_interaction_coverage]
            interaction_coverage_labels = ['Main 3 Categories', 'Other Interactions']
            
            ax6.pie(interaction_coverage_data, labels=interaction_coverage_labels, autopct='%1.1f%%', 
                   colors=coverage_colors, startangle=90)
            ax6.set_title('Interaction Coverage', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"  ğŸ’¾ Analysis plots saved to: {save_path}")
        
        plt.show()
    
    def generate_report(self, metrics: Dict, scene_analysis: Dict, output_path: str = None):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print(f"\nğŸ“ Generating analysis report...")
        
        report = []
        report.append("# JRDB Label Co-occurrence Analysis Report")
        report.append("=" * 50)
        
        # æ€»ä½“ç»Ÿè®¡
        overall = metrics['overall']
        report.append(f"\n## Overall Statistics")
        report.append(f"- Total interactions: {overall['total_interactions']:,}")
        report.append(f"- Unique interaction types: {overall['unique_labels']}")
        report.append(f"- Main three categories coverage: {overall['main_three_coverage']:.2f}%")
        
        # ä¸»è¦ä¸‰ç±»åˆ†æ
        report.append(f"\n## Main Categories Analysis")
        for category_key, category_data in metrics.items():
            if category_key in ['overall', 'interaction_label_stats']:
                continue
                
            report.append(f"\n### {category_data['label']}")
            report.append(f"- Label count: {category_data['count']:,} ({category_data['percentage']:.2f}%)")
            report.append(f"- Co-occurrence rate: {category_data['co_occurrence_rate']:.2f}%")
            report.append(f"- Solo appearances: {category_data['solo_appearances']:,}")
            
            # æ·»åŠ interactionæ¯”ä¾‹ä¿¡æ¯
            if 'interaction_label_stats' in metrics:
                interaction_stats = metrics['interaction_label_stats']
                label_name = category_data['label']
                if label_name in interaction_stats:
                    interaction_info = interaction_stats[label_name]
                    report.append(f"- Interaction coverage: {interaction_info['interaction_count']:,} interactions ({interaction_info['proportion']:.2f}%)")
                else:
                    report.append(f"- Interaction coverage: 0 interactions (0.00%)")
            
            if category_data['top_co_labels']:
                report.append(f"- Top co-occurring labels:")
                for label, count in list(category_data['top_co_labels'].items())[:3]:
                    report.append(f"  - {label}: {count:,}")
        
        # åœºæ™¯åˆ†ææ‘˜è¦
        report.append(f"\n## Scene Analysis Summary")
        report.append(f"- Total scenes analyzed: {len(scene_analysis)}")
        
        # æ‰¾å‡ºæœ€æœ‰ç‰¹è‰²çš„åœºæ™¯
        distinctive_scenes = []
        for scene_name, analysis in scene_analysis.items():
            if analysis['distinctive_labels']:
                distinctive_scenes.append((scene_name, len(analysis['distinctive_labels'])))
        
        distinctive_scenes.sort(key=lambda x: x[1], reverse=True)
        
        if distinctive_scenes:
            report.append(f"- Most distinctive scenes:")
            for scene_name, distinctive_count in distinctive_scenes[:5]:
                report.append(f"  - {scene_name}: {distinctive_count} distinctive labels")
        
        # å…³é”®å‘ç°
        report.append(f"\n## Key Findings")
        
        # 1. å¤šçº§åˆ†ç±»åˆç†æ€§
        main_coverage = overall['main_three_coverage']
        if main_coverage > 80:
            report.append(f"âœ… **Multi-level classification is well-justified**: Main 3 categories cover {main_coverage:.1f}% of all interactions")
        else:
            report.append(f"âš ï¸ **Consider expanding main categories**: Only {main_coverage:.1f}% coverage")
        
        # 2. æ ‡ç­¾å¤æ‚æ€§
        avg_co_occurrence = np.mean([metrics[cat]['co_occurrence_rate'] for cat in self.main_categories.keys() 
                                   if cat in metrics])
        if avg_co_occurrence > 30:
            report.append(f"âœ… **Multi-label modeling recommended**: Average co-occurrence rate is {avg_co_occurrence:.1f}%")
        else:
            report.append(f"ğŸ“ **Single-label may suffice**: Low co-occurrence rate ({avg_co_occurrence:.1f}%)")
        
        # 3. åœºæ™¯ç‰¹å¼‚æ€§
        if len(distinctive_scenes) > len(scene_analysis) * 0.3:
            report.append(f"âœ… **Scene-specific adaptation needed**: {len(distinctive_scenes)} scenes show distinctive patterns")
        
        report_text = "\n".join(report)
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"  ğŸ’¾ Report saved to: {output_path}")
        
        print("\n" + report_text)
        return report_text


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='JRDB Label Co-occurrence Analysis')
    
    parser.add_argument('--data_path', type=str, default='../dataset',
                       help='Path to JRDB dataset (contains labels/)')
    parser.add_argument('--output_dir', type=str, default='./analysis_results',
                       help='Output directory for results')
    parser.add_argument('--top_n', type=int, default=15,
                       help='Number of top labels to include in visualizations')
    parser.add_argument('--save_plots', action='store_true',
                       help='Save plots to files')
    parser.add_argument('--generate_chord', action='store_true',
                       help='Generate chord diagram (requires chord package)')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print(f"JRDB Label Co-occurrence Analysis")
    print(f"Data path: {args.data_path}")
    print(f"Output directory: {output_dir}")
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = JRDBLabelAnalyzer(args.data_path)
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        total_interactions = analyzer.load_and_process_data()
        
        if total_interactions == 0:
            print("âŒ No interaction data found. Please check data path.")
            return
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        metrics = analyzer.calculate_key_metrics()
        
        # åˆ›å»ºå…±ç°çŸ©é˜µ
        co_matrix = analyzer.create_co_occurrence_matrix(args.top_n)
        
        # ç”Ÿæˆå¯è§†åŒ–
        print(f"\nğŸ¨ Generating visualizations...")
        
        # çƒ­åŠ›å›¾
        heatmap_path = output_dir / "co_occurrence_heatmap.png" if args.save_plots else None
        analyzer.plot_heatmap(co_matrix, heatmap_path)
        
        # ä¸»è¦ç±»åˆ«åˆ†æå›¾
        analysis_path = output_dir / "main_categories_analysis.png" if args.save_plots else None
        analyzer.plot_main_categories_analysis(metrics, analysis_path)
        
        # å’Œå¼¦å›¾ï¼ˆå¯é€‰ï¼‰
        if args.generate_chord:
            chord_path = output_dir / "chord_diagram.html"
            analyzer.create_chord_diagram(args.top_n, chord_path)
        
        # åœºæ™¯åˆ†æ
        scene_analysis = analyzer.analyze_scene_patterns()
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = output_dir / "analysis_report.md"
        analyzer.generate_report(metrics, scene_analysis, report_path)
        
        # ä¿å­˜æ•°æ®
        if args.save_plots:
            # ä¿å­˜å…±ç°çŸ©é˜µ
            matrix_path = output_dir / "co_occurrence_matrix.csv"
            co_matrix.to_csv(matrix_path)
            print(f"ğŸ’¾ Co-occurrence matrix saved to: {matrix_path}")
            
            # ä¿å­˜æŒ‡æ ‡æ•°æ®
            import json
            metrics_path = output_dir / "metrics.json"
            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Metrics saved to: {metrics_path}")
        
        print(f"\nâœ… Analysis completed successfully!")
        print(f"Check results in: {output_dir}")
        
    except Exception as e:
        print(f"âŒ Analysis failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()