#!/usr/bin/env python3
"""
LSTMæ¨¡å¼æµ‹è¯•è„šæœ¬
æµ‹è¯•LSTMæ•°æ®é›†ã€æ¨¡å‹å’Œè®­ç»ƒæµç¨‹
"""

import os
import sys

# è§£å†³OpenMPåº“å†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from configs.stage2_config import Stage2Config
from utils.model_factory import create_stage2_model, create_stage2_loss
from utils.data_factory import create_stage2_data_loaders
from datasets.stage2_dataset import LSTMStage2Dataset


def test_lstm_config():
    """æµ‹è¯•LSTMé…ç½®"""
    print("ğŸ”§ Testing LSTM Configuration...")
    
    config = Stage2Config(
        temporal_mode='lstm',           # LSTMæ¨¡å¼
        use_geometric=True,
        use_hog=True,
        use_scene_context=True,
        sequence_length=5,              # 5å¸§åºåˆ—
        lstm_hidden_dim=64,
        lstm_layers=2,
        bidirectional=True,
        hidden_dims=[64, 32],
        dropout=0.2,
        batch_size=4,                   # å°æ‰¹æ¬¡ä¾¿äºæµ‹è¯•
        data_path="../dataset"
    )
    
    config.validate()
    print(f"âœ… LSTM Config validated:")
    print(f"  Mode: {config.temporal_mode}")
    print(f"  Sequence length: {config.sequence_length}")
    print(f"  Input dim: {config.get_input_dim()}")
    print(f"  LSTM config: {config.lstm_layers} layers, {config.lstm_hidden_dim} hidden, bidirectional={config.bidirectional}")
    
    return config


def test_lstm_model(config):
    """æµ‹è¯•LSTMæ¨¡å‹"""
    print(f"\nğŸ§  Testing LSTM Model...")
    
    try:
        model = create_stage2_model(config)
        model_info = model.get_model_info()
        print(f"âœ… LSTM Model created:")
        print(f"  Type: {model_info['model_type']}")
        print(f"  Parameters: {model_info['trainable_params']:,}")
        print(f"  Feature dim: {model_info['feature_dim']}")
        print(f"  Sequence length: {model_info['sequence_length']}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 2
        seq_len = config.sequence_length
        feat_dim = config.get_input_dim()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ [batch_size, sequence_length, feature_dim]
        test_input = torch.randn(batch_size, seq_len, feat_dim)
        print(f"\nğŸ“Š Testing forward pass:")
        print(f"  Input shape: {test_input.shape}")
        
        with torch.no_grad():
            output = model(test_input)
            print(f"  Output shape: {output.shape}")
            print(f"  Output range: [{output.min().item():.3f}, {output.max().item():.3f}]")
        
        return model
        
    except Exception as e:
        print(f"âŒ LSTM model test failed: {e}")
        raise


def test_lstm_dataset(config):
    """æµ‹è¯•LSTMæ•°æ®é›†"""
    print(f"\nğŸ“š Testing LSTM Dataset...")
    
    try:
        # åˆ›å»ºå°æ•°æ®é›†è¿›è¡Œæµ‹è¯•
        dataset = LSTMStage2Dataset(
            data_path=config.data_path,
            split='train',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            sequence_length=config.sequence_length,
            frame_interval=config.frame_interval,
            use_oversampling=False  # æµ‹è¯•æ—¶ä¸ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
        print(f"âœ… LSTM Dataset created:")
        print(f"  Total sequences: {len(dataset)}")
        
        if len(dataset) > 0:
            # æµ‹è¯•æ ·æœ¬åŠ è½½
            sample = dataset[0]
            print(f"\nğŸ“‹ Sample structure:")
            print(f"  Keys: {sample.keys()}")
            print(f"  Sequences shape: {sample['sequences'].shape}")
            print(f"  Label: {sample['stage2_label'].item()}")
            print(f"  Group key: {sample['group_key']}")
            print(f"  Frame range: {sample['start_frame']} - {sample['end_frame']}")
            
            # æµ‹è¯•å¤šä¸ªæ ·æœ¬
            print(f"\nğŸ“Š Testing multiple samples:")
            for i in range(min(3, len(dataset))):
                sample = dataset[i]
                print(f"  Sample {i}: sequences={sample['sequences'].shape}, label={sample['stage2_label'].item()}")
            
            # ç±»åˆ«åˆ†å¸ƒ
            distribution = dataset.get_class_distribution()
            print(f"\nğŸ“ˆ Class distribution: {distribution}")
        
        return dataset
        
    except FileNotFoundError as e:
        print(f"âš ï¸  Dataset path not found: {e}")
        print("This is expected if the dataset doesn't exist")
        return None
    except Exception as e:
        print(f"âŒ LSTM dataset test failed: {e}")
        raise


def test_lstm_data_loaders(config):
    """æµ‹è¯•LSTMæ•°æ®åŠ è½½å™¨"""
    print(f"\nğŸ”„ Testing LSTM Data Loaders...")
    
    try:
        train_loader, val_loader, test_loader = create_stage2_data_loaders(config)
        
        print(f"âœ… Data loaders created:")
        print(f"  Train: {len(train_loader.dataset)} sequences, {len(train_loader)} batches")
        print(f"  Val: {len(val_loader.dataset)} sequences, {len(val_loader)} batches")
        print(f"  Test: {len(test_loader.dataset)} sequences, {len(test_loader)} batches")
        
        # æµ‹è¯•æ‰¹æ¬¡åŠ è½½
        if len(train_loader) > 0:
            print(f"\nğŸ¯ Testing batch loading:")
            for batch_idx, batch in enumerate(train_loader):
                if batch_idx >= 2:  # åªæµ‹è¯•å‰2ä¸ªæ‰¹æ¬¡
                    break
                
                sequences = batch['sequences']  # [batch_size, seq_len, feat_dim]
                labels = batch['stage2_label']   # [batch_size]
                
                print(f"  Batch {batch_idx}:")
                print(f"    Sequences: {sequences.shape}")
                print(f"    Labels: {labels.shape}, unique: {torch.unique(labels).tolist()}")
        
        return train_loader, val_loader, test_loader
        
    except FileNotFoundError as e:
        print(f"âš ï¸  Dataset path not found: {e}")
        return None, None, None
    except Exception as e:
        print(f"âŒ Data loader test failed: {e}")
        raise


def test_lstm_loss(config, model):
    """æµ‹è¯•LSTMæŸå¤±å‡½æ•°"""
    print(f"\nğŸ’¥ Testing LSTM Loss Function...")
    
    try:
        criterion = create_stage2_loss(config)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        seq_len = config.sequence_length
        feat_dim = config.get_input_dim()
        
        test_sequences = torch.randn(batch_size, seq_len, feat_dim)
        test_labels = torch.randint(0, 3, (batch_size,))
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            logits = model(test_sequences)
            loss, loss_dict = criterion(logits, test_labels)
        
        print(f"âœ… Loss computation successful:")
        print(f"  Total loss: {loss.item():.4f}")
        print(f"  Loss details: {loss_dict}")
        
        return criterion
        
    except Exception as e:
        print(f"âŒ LSTM loss test failed: {e}")
        raise


def test_training_step(config, model, criterion, data_loader):
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print(f"\nğŸƒ Testing Training Step...")
    
    if data_loader is None or len(data_loader) == 0:
        print("âš ï¸  No data loader available for training test")
        return
    
    try:
        model.train()
        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
        
        # è¿è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤
        batch = next(iter(data_loader))
        sequences = batch['sequences']
        labels = batch['stage2_label']
        
        print(f"  Training batch:")
        print(f"    Input: {sequences.shape}")
        print(f"    Labels: {labels.shape}")
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        logits = model(sequences)
        loss, loss_dict = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        print(f"âœ… Training step completed:")
        print(f"  Loss: {loss.item():.4f}")
        print(f"  Gradients: {'OK' if any(p.grad is not None for p in model.parameters()) else 'Missing'}")
        
    except Exception as e:
        print(f"âŒ Training step test failed: {e}")
        raise


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ” Starting LSTM Mode Comprehensive Test...")
    print("=" * 60)
    
    try:
        # 1. æµ‹è¯•é…ç½®
        config = test_lstm_config()
        
        # 2. æµ‹è¯•æ¨¡å‹
        model = test_lstm_model(config)
        
        # 3. æµ‹è¯•æŸå¤±å‡½æ•°
        criterion = test_lstm_loss(config, model)
        
        # 4. æµ‹è¯•æ•°æ®é›†
        dataset = test_lstm_dataset(config)
        
        # 5. æµ‹è¯•æ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = test_lstm_data_loaders(config)
        
        # 6. æµ‹è¯•è®­ç»ƒæ­¥éª¤
        test_training_step(config, model, criterion, train_loader)
        
        print(f"\n" + "=" * 60)
        print("âœ… LSTM Mode Test Completed Successfully!")
        print("ğŸš€ Ready for LSTM training!")
        
        # å‚æ•°é‡å¯¹æ¯”
        basic_config = Stage2Config(temporal_mode='none')
        basic_params = 19779  # ä»ä¹‹å‰è®¡ç®—å¾—å‡º
        lstm_params = model.get_model_info()['trainable_params']
        
        print(f"\nğŸ“Š Parameter Comparison:")
        print(f"  Basic mode: {basic_params:,} parameters")
        print(f"  LSTM mode:  {lstm_params:,} parameters")
        print(f"  Ratio: {lstm_params/basic_params:.1f}x")
        
    except Exception as e:
        print(f"\nâŒ LSTM Mode Test Failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()