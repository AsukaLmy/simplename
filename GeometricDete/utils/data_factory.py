#!/usr/bin/env python3
"""
Data Factory for Stage2 Behavior Classification
Creates appropriate datasets and data loaders based on configuration
"""

from torch.utils.data import DataLoader
from typing import Tuple
import os

from configs.stage2_config import Stage2Config


def create_stage2_data_loaders(config: Stage2Config) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    åˆ›å»ºStage2æ•°æ®åŠ è½½å™¨
    
    Args:
        config: Stage2é…ç½®å¯¹è±¡
        
    Returns:
        Tuple[DataLoader, DataLoader, DataLoader]: (train_loader, val_loader, test_loader)
    """
    
    if config.temporal_mode == 'none':
        # Basicæ¨¡å¼ - ä½¿ç”¨é‡æ„çš„æ•°æ®é›†
        from datasets.stage2_dataset import BasicStage2Dataset
        
        print(f"ğŸ”„ Creating Basic mode datasets...")
        print(f"   Features: {'Geometric(7)' if config.use_geometric else ''}"
              f"{' + HoG(64)' if config.use_hog else ''}"
              f"{' + Scene(1)' if config.use_scene_context else ''}")
        print(f"   Frame interval: {config.frame_interval}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = BasicStage2Dataset(
            data_path=config.data_path,
            split='train',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            frame_interval=config.frame_interval,
            use_oversampling=True  # è®­ç»ƒé›†ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
        val_dataset = BasicStage2Dataset(
            data_path=config.data_path,
            split='val',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            frame_interval=config.frame_interval,
            use_oversampling=False  # éªŒè¯é›†ä¸ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
        test_dataset = BasicStage2Dataset(
            data_path=config.data_path,
            split='test',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            frame_interval=config.frame_interval,
            use_oversampling=False  # æµ‹è¯•é›†ä¸ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
    elif config.temporal_mode == 'lstm':
        # LSTMæ¨¡å¼ - ä½¿ç”¨æ—¶åºæ•°æ®é›†
        from datasets.stage2_dataset import LSTMStage2Dataset
        
        print(f"ğŸ”„ Creating LSTM mode datasets...")
        print(f"   Features: {'Geometric(7)' if config.use_geometric else ''}"
              f"{' + HoG(64)' if config.use_hog else ''}"
              f"{' + Scene(1)' if config.use_scene_context else ''}")
        print(f"   Sequence length: {config.sequence_length}")
        print(f"   Frame interval: {config.frame_interval}")
        
        # åˆ›å»ºæ—¶åºæ•°æ®é›†
        train_dataset = LSTMStage2Dataset(
            data_path=config.data_path,
            split='train',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            sequence_length=config.sequence_length,
            frame_interval=config.frame_interval,
            use_oversampling=True  # è®­ç»ƒé›†ä½¿ç”¨æ—¶åºè¿‡é‡‡æ ·
        )
        
        val_dataset = LSTMStage2Dataset(
            data_path=config.data_path,
            split='val',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            sequence_length=config.sequence_length,
            frame_interval=config.frame_interval,
            use_oversampling=False  # éªŒè¯é›†ä¸ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
        test_dataset = LSTMStage2Dataset(
            data_path=config.data_path,
            split='test',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            sequence_length=config.sequence_length,
            frame_interval=config.frame_interval,
            use_oversampling=False  # æµ‹è¯•é›†ä¸ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
    elif config.temporal_mode == 'relation':
        # Relation Networkæ¨¡å¼ - ä½¿ç”¨å…³ç³»ç½‘ç»œæ•°æ®é›†
        from datasets.stage2_dataset import RelationStage2Dataset
        
        print(f"ğŸ”„ Creating Relation Network mode datasets...")
        print(f"   Features: Person={'HoG(32)' if config.use_hog else 'None'}")
        print(f"   Spatial: {'Geometric(7)' if config.use_geometric else ''}"
              f"{' + Scene(1)' if config.use_scene_context else ''}")
        print(f"   Fusion strategy: {config.fusion_strategy}")
        print(f"   Frame interval: {config.frame_interval}")
        
        # åˆ›å»ºå…³ç³»ç½‘ç»œæ•°æ®é›†
        train_dataset = RelationStage2Dataset(
            data_path=config.data_path,
            split='train',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            frame_interval=config.frame_interval,
            use_oversampling=True  # è®­ç»ƒé›†ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
        val_dataset = RelationStage2Dataset(
            data_path=config.data_path,
            split='val',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            frame_interval=config.frame_interval,
            use_oversampling=False  # éªŒè¯é›†ä¸ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
        test_dataset = RelationStage2Dataset(
            data_path=config.data_path,
            split='test',
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context,
            frame_interval=config.frame_interval,
            use_oversampling=False  # æµ‹è¯•é›†ä¸ä½¿ç”¨è¿‡é‡‡æ ·
        )
        
    else:
        raise ValueError(f"Unknown temporal_mode: {config.temporal_mode}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=True,
        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print(f"âœ… Data loaders created:")
    print(f"   Train: {len(train_dataset):,} samples, {len(train_loader)} batches")
    print(f"   Val:   {len(val_dataset):,} samples, {len(val_loader)} batches")
    print(f"   Test:  {len(test_dataset):,} samples, {len(test_loader)} batches")
    print(f"   Feature dimension: {config.get_input_dim()}D")
    
    return train_loader, val_loader, test_loader


def get_dataset_statistics(data_loaders: Tuple[DataLoader, DataLoader, DataLoader]) -> dict:
    """
    è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        data_loaders: (train_loader, val_loader, test_loader)
        
    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯
    """
    train_loader, val_loader, test_loader = data_loaders
    
    # è·å–ç±»åˆ«åˆ†å¸ƒ
    def get_class_distribution(dataset):
        if hasattr(dataset, 'get_class_distribution'):
            return dataset.get_class_distribution()
        else:
            return {"message": "Class distribution not available"}
    
    train_dist = get_class_distribution(train_loader.dataset)
    val_dist = get_class_distribution(val_loader.dataset)
    test_dist = get_class_distribution(test_loader.dataset)
    
    # è·å–æ ·æœ¬ç»Ÿè®¡
    def get_sample_info(dataset):
        try:
            sample = dataset[0]
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®é›†è¾“å‡ºæ ¼å¼
            if 'features' in sample:
                # Basic/LSTMæ¨¡å¼
                feature_shape = sample['features'].shape
                return {
                    'sample_count': len(dataset),
                    'feature_shape': list(feature_shape),
                    'feature_dim': feature_shape[0] if len(feature_shape) == 1 else feature_shape[-1]
                }
            elif 'sequences' in sample:
                # LSTMæ¨¡å¼
                seq_shape = sample['sequences'].shape
                return {
                    'sample_count': len(dataset),
                    'sequence_shape': list(seq_shape),
                    'feature_dim': seq_shape[-1] if len(seq_shape) > 1 else seq_shape[0]
                }
            elif 'person_A_features' in sample and 'person_B_features' in sample:
                # Relation Networkæ¨¡å¼
                person_A_shape = sample['person_A_features'].shape
                person_B_shape = sample['person_B_features'].shape
                spatial_shape = sample['spatial_features'].shape if sample['spatial_features'].numel() > 0 else [0]
                return {
                    'sample_count': len(dataset),
                    'person_A_shape': list(person_A_shape),
                    'person_B_shape': list(person_B_shape), 
                    'spatial_shape': list(spatial_shape),
                    'person_feature_dim': person_A_shape[0] if len(person_A_shape) == 1 else person_A_shape[-1],
                    'spatial_feature_dim': spatial_shape[0] if len(spatial_shape) == 1 else spatial_shape[-1]
                }
            else:
                return {'error': 'Unknown dataset output format'}
        except Exception as e:
            return {'error': str(e)}
    
    train_info = get_sample_info(train_loader.dataset)
    val_info = get_sample_info(val_loader.dataset)
    test_info = get_sample_info(test_loader.dataset)
    
    statistics = {
        'train': {
            'distribution': train_dist,
            'info': train_info
        },
        'val': {
            'distribution': val_dist,
            'info': val_info
        },
        'test': {
            'distribution': test_dist,
            'info': test_info
        },
        'total_samples': train_info.get('sample_count', 0) + val_info.get('sample_count', 0) + test_info.get('sample_count', 0)
    }
    
    return statistics


def print_dataset_summary(config: Stage2Config, data_loaders: Tuple[DataLoader, DataLoader, DataLoader]):
    """
    æ‰“å°æ•°æ®é›†æ‘˜è¦ä¿¡æ¯
    
    Args:
        config: é…ç½®å¯¹è±¡
        data_loaders: æ•°æ®åŠ è½½å™¨å…ƒç»„
    """
    print("\n" + "="*60)
    print("STAGE2 DATASET SUMMARY")
    print("="*60)
    
    print(f"Mode: {config.temporal_mode.upper()}")
    print(f"Data path: {config.data_path}")
    print(f"Frame interval: {config.frame_interval}")
    print(f"Batch size: {config.batch_size}")
    
    # ç‰¹å¾é…ç½®
    features = []
    if config.use_geometric:
        features.append("Geometric(7)")
    if config.use_hog:
        features.append("HoG(64)")
    if config.use_scene_context:
        features.append("Scene(1)")
    
    print(f"Features: {' + '.join(features)}")
    print(f"Total feature dimension: {config.get_input_dim()}D")
    
    # æ•°æ®ç»Ÿè®¡
    try:
        stats = get_dataset_statistics(data_loaders)
        train_loader, val_loader, test_loader = data_loaders
        
        print(f"\nDataset Sizes:")
        print(f"  Train: {len(train_loader.dataset):,} samples ({len(train_loader)} batches)")
        print(f"  Val:   {len(val_loader.dataset):,} samples ({len(val_loader)} batches)")  
        print(f"  Test:  {len(test_loader.dataset):,} samples ({len(test_loader)} batches)")
        print(f"  Total: {stats['total_samples']:,} samples")
        
        # ç±»åˆ«åˆ†å¸ƒ (å¦‚æœå¯ç”¨)
        train_dist = stats['train']['distribution']
        if 'class_counts' in train_dist:
            print(f"\nTrain Class Distribution:")
            class_names = train_dist.get('class_names', [f'Class_{i}' for i in range(3)])
            for i, (class_id, count) in enumerate(train_dist['class_counts'].items()):
                class_name = class_names[i] if i < len(class_names) else f'Class_{class_id}'
                percentage = 100 * count / train_dist['total'] if train_dist['total'] > 0 else 0
                print(f"  {class_name}: {count:,} ({percentage:.1f}%)")
                
    except Exception as e:
        print(f"\nWarning: Could not gather detailed statistics: {e}")
    
    print("="*60)


if __name__ == '__main__':
    # æµ‹è¯•æ•°æ®å·¥å‚
    print("Testing Data Factory...")
    
    # ç”±äºéœ€è¦å®é™…æ•°æ®é›†ï¼Œè¿™é‡Œåªæµ‹è¯•é…ç½®éªŒè¯
    from configs.stage2_config import Stage2Config
    
    print("\n1. Testing Basic mode configuration...")
    config_basic = Stage2Config(
        temporal_mode='none',
        data_path="../dataset",  # å‡è®¾è·¯å¾„
        use_geometric=True,
        use_hog=True,
        use_scene_context=True,
        batch_size=32,
        frame_interval=1
    )
    
    print(f"Config validation:")
    config_basic.validate()
    print(f"  Mode: {config_basic.temporal_mode}")
    print(f"  Input dimension: {config_basic.get_input_dim()}")
    print(f"  Features: Geometric={config_basic.use_geometric}, HoG={config_basic.use_hog}, Scene={config_basic.use_scene_context}")
    
    print(f"\n2. Testing frame interval configuration...")
    config_sparse = Stage2Config(
        temporal_mode='none',
        data_path="../dataset",
        frame_interval=10,  # æ¯10å¸§é‡‡æ ·
        batch_size=64
    )
    
    config_sparse.validate()
    print(f"  Frame interval: {config_sparse.frame_interval}")
    print(f"  Expected sample reduction: ~90%")
    
    # æ³¨æ„ï¼šå®é™…çš„æ•°æ®åŠ è½½å™¨åˆ›å»ºéœ€è¦å­˜åœ¨çš„æ•°æ®é›†
    print(f"\n3. Data loader creation would require existing dataset at: {config_basic.data_path}")
    print("   Use create_stage2_data_loaders() when dataset is available")
    
    print("\nâœ… Data factory test completed!")