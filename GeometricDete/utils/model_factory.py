#!/usr/bin/env python3
"""
Model Factory for Stage2 Behavior Classification
Creates appropriate models based on configuration
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple, Optional

# å¯¼å…¥é…ç½®å’Œæ¨¡å‹ç±»
from configs.stage2_config import Stage2Config
from models.stage2_classifier import BasicStage2Classifier, LSTMStage2Classifier, RelationStage2Classifier, Stage2Loss


def create_stage2_model(config: Stage2Config) -> nn.Module:
    """
    æ ¹æ®é…ç½®åˆ›å»ºStage2æ¨¡å‹
    
    Args:
        config: Stage2é…ç½®å¯¹è±¡
        
    Returns:
        nn.Module: åˆ›å»ºçš„æ¨¡å‹
    """
    input_dim = config.get_input_dim()
    
    if config.temporal_mode == 'none':
        # Basicæ¨¡å¼
        model = BasicStage2Classifier(
            input_dim=input_dim,
            hidden_dims=config.hidden_dims,
            dropout=config.dropout,
            use_attention=config.use_attention
        )
        print(f"âœ… Created BasicStage2Classifier: {input_dim}D â†’ 3 classes")
        
    elif config.temporal_mode == 'lstm':
        # LSTMæ¨¡å¼
        model = LSTMStage2Classifier(
            feature_dim=input_dim,
            sequence_length=config.sequence_length,
            lstm_hidden_dim=config.lstm_hidden_dim,
            lstm_layers=config.lstm_layers,
            bidirectional=config.bidirectional,
            hidden_dims=config.hidden_dims,
            dropout=config.dropout
        )
        print(f"âœ… Created LSTMStage2Classifier: {input_dim}DÃ—{config.sequence_length} â†’ 3 classes")
        
    elif config.temporal_mode == 'relation':
        # Relation Networkæ¨¡å¼ - éœ€è¦ä»RelationFeatureFusionè·å–æ­£ç¡®çš„ç»´åº¦
        from models.feature_extractors import RelationFeatureFusion
        temp_fusion = RelationFeatureFusion(
            use_geometric=config.use_geometric,
            use_hog=config.use_hog,
            use_scene_context=config.use_scene_context
        )
        
        person_feature_dim = temp_fusion.get_person_feature_dim()
        spatial_feature_dim = temp_fusion.get_spatial_feature_dim()
        # Validate person feature dim - relation mode requires per-person features (e.g., HoG)
        if person_feature_dim <= 0:
            raise ValueError(
                "Relation mode requires non-zero per-person features (enable HoG or adjust RelationFeatureFusion). "
                f"Got person_feature_dim={person_feature_dim}."
            )
        model = RelationStage2Classifier(
            person_feature_dim=person_feature_dim,  # æ¯ä¸ªäººçš„ä¸ªä½“ç‰¹å¾ç»´åº¦
            spatial_feature_dim=spatial_feature_dim,  # ç©ºé—´å…³ç³»ç‰¹å¾ç»´åº¦
            hidden_dims=config.relation_hidden_dims,
            dropout=config.dropout,
            fusion_strategy=config.fusion_strategy
        )
        print(f"Created RelationStage2Classifier: Person({person_feature_dim}D)x2 + Spatial({spatial_feature_dim}D) -> 3 classes ({config.fusion_strategy})")
        
    else:
        raise ValueError(f"Unknown temporal_mode: {config.temporal_mode}")
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    model_info = model.get_model_info()
    print(f"Model parameters: {model_info['trainable_params']:,}")
    print(f"Model structure: {model_info['hidden_dims']}")
    
    return model


def create_stage2_loss(config: Stage2Config) -> Stage2Loss:
    """
    åˆ›å»ºStage2æŸå¤±å‡½æ•°
    
    Args:
        config: Stage2é…ç½®å¯¹è±¡
        
    Returns:
        Stage2Loss: æŸå¤±å‡½æ•°
    """
    # å¦‚æœconfigä¸­æ²¡æœ‰class_weightsï¼Œæä¾›å‡åŒ€æƒé‡ä½œä¸ºå…œåº•
    if not hasattr(config, 'class_weights') or config.class_weights is None:
        # å‡è®¾3ç±»å‡åŒ€æƒé‡
        config.class_weights = {0: 1.0, 1: 1.0, 2: 1.0}
        print("âš ï¸ config.class_weights not found â€” using uniform fallback weights: {0:1.0,1:1.0,2:1.0}")

    criterion = Stage2Loss(
        class_weights=config.class_weights,
        mpca_weight=config.mpca_weight,
        acc_weight=config.acc_weight
    )

    print(f"âœ… Created Stage2Loss: weights={config.class_weights}")
    return criterion


def create_optimizer(model: nn.Module, config: Stage2Config) -> optim.Optimizer:
    """
    åˆ›å»ºä¼˜åŒ–å™¨
    
    Args:
        model: æ¨¡å‹
        config: é…ç½®å¯¹è±¡
        
    Returns:
        torch.optim.Optimizer: ä¼˜åŒ–å™¨
    """
    if config.optimizer == 'adam':
        optimizer = optim.Adam(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
    elif config.optimizer == 'sgd':
        optimizer = optim.SGD(
            model.parameters(),
            lr=config.learning_rate,
            momentum=0.9,
            weight_decay=config.weight_decay
        )
    else:
        raise ValueError(f"Unknown optimizer: {config.optimizer}")
    
    print(f"âœ… Created optimizer: {config.optimizer}, lr={config.learning_rate}")
    return optimizer


def create_scheduler(optimizer: optim.Optimizer, config: Stage2Config) -> Optional[optim.lr_scheduler._LRScheduler]:
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        config: é…ç½®å¯¹è±¡
        
    Returns:
        Optional[_LRScheduler]: å­¦ä¹ ç‡è°ƒåº¦å™¨ (å¯èƒ½ä¸ºNone)
    """
    if config.scheduler == 'step':
        scheduler = optim.lr_scheduler.StepLR(
            optimizer, step_size=config.step_size, gamma=0.1
        )
    elif config.scheduler == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config.epochs
        )
    elif config.scheduler == 'plateau':
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=5, factor=0.5
        )
    elif config.scheduler == 'none':
        scheduler = None
    else:
        raise ValueError(f"Unknown scheduler: {config.scheduler}")
    
    if scheduler:
        print(f"âœ… Created scheduler: {config.scheduler}")
    else:
        print("âœ… No scheduler used")
        
    return scheduler


def create_full_training_setup(config: Stage2Config, device: torch.device) -> Tuple[nn.Module, Stage2Loss, optim.Optimizer, Optional[optim.lr_scheduler._LRScheduler]]:
    """
    åˆ›å»ºå®Œæ•´çš„è®­ç»ƒè®¾ç½®
    
    Args:
        config: é…ç½®å¯¹è±¡
        device: è®¾å¤‡
        
    Returns:
        Tuple: (model, criterion, optimizer, scheduler)
    """
    print(f"\nğŸ—ï¸ Creating training setup for {config.model_type}...")
    
    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    model = create_stage2_model(config).to(device)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    criterion = create_stage2_loss(config).to(device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config)
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = create_scheduler(optimizer, config)
    
    print(f"âœ… Training setup completed on {device}")
    
    return model, criterion, optimizer, scheduler


class ModelCheckpointManager:
    """æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, save_dir: str):
        self.save_dir = save_dir
    
    def save_checkpoint(self, model: nn.Module, optimizer: optim.Optimizer, 
                       scheduler: Optional[optim.lr_scheduler._LRScheduler],
                       epoch: int, metrics: dict, filename: str, config: Stage2Config):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            scheduler: è°ƒåº¦å™¨
            epoch: å½“å‰epoch
            metrics: è¯„ä¼°æŒ‡æ ‡
            filename: æ–‡ä»¶å
            config: é…ç½®
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics': metrics,
            'config': config.__dict__,
            'model_info': model.get_model_info() if hasattr(model, 'get_model_info') else {}
        }
        
        filepath = f"{self.save_dir}/{filename}.pth"
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Checkpoint saved: {filename}.pth")
    
    def load_checkpoint(self, filepath: str, model: nn.Module, 
                       optimizer: Optional[optim.Optimizer] = None,
                       scheduler: Optional[optim.lr_scheduler._LRScheduler] = None) -> dict:
        """
        åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            filepath: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨ (å¯é€‰)
            scheduler: è°ƒåº¦å™¨ (å¯é€‰)
            
        Returns:
            dict: æ£€æŸ¥ç‚¹ä¿¡æ¯
        """
        checkpoint = torch.load(filepath, map_location='cpu')
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        print(f"ğŸ“‚ Checkpoint loaded from: {filepath}")
        print(f"Epoch: {checkpoint.get('epoch', 'Unknown')}")
        print(f"Metrics: {checkpoint.get('metrics', {})}")
        
        return checkpoint


if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å‹å·¥å‚
    print("Testing Model Factory...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    from configs.stage2_config import Stage2Config
    
    print("\n1. Testing Basic mode model creation...")
    config_basic = Stage2Config(
        temporal_mode='none',
        use_geometric=True,
        use_hog=True,
        use_scene_context=True,
        hidden_dims=[64, 32, 16],
        dropout=0.2
    )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model, criterion, optimizer, scheduler = create_full_training_setup(config_basic, device)
    
    print(f"\nModel type: {type(model).__name__}")
    print(f"Loss type: {type(criterion).__name__}")
    print(f"Optimizer type: {type(optimizer).__name__}")
    print(f"Scheduler type: {type(scheduler).__name__ if scheduler else 'None'}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\n2. Testing forward pass...")
    batch_size = 4
    input_dim = config_basic.get_input_dim()
    test_input = torch.randn(batch_size, input_dim).to(device)
    test_targets = torch.randint(0, 3, (batch_size,)).to(device)
    
    with torch.no_grad():
        logits = model(test_input)
        loss, loss_dict = criterion(logits, test_targets)
    
    print(f"Input shape: {test_input.shape}")
    print(f"Output shape: {logits.shape}")
    print(f"Loss: {loss.item():.4f}")
    
    # æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    print(f"\n3. Testing checkpoint manager...")
    import tempfile
    import os
    
    with tempfile.TemporaryDirectory() as temp_dir:
        checkpoint_manager = ModelCheckpointManager(temp_dir)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        test_metrics = {'val_accuracy': 0.85, 'val_mpca': 0.82}
        checkpoint_manager.save_checkpoint(
            model, optimizer, scheduler, 
            epoch=10, metrics=test_metrics, 
            filename='test_checkpoint', config=config_basic
        )
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(temp_dir, 'test_checkpoint.pth')
        loaded_checkpoint = checkpoint_manager.load_checkpoint(
            checkpoint_path, model, optimizer, scheduler
        )
        
        print(f"Loaded checkpoint epoch: {loaded_checkpoint.get('epoch')}")
    
    print("\nâœ… Model factory test completed!")