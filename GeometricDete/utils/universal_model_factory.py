#!/usr/bin/env python3
"""
Universal Model Factory for Stage2 Behavior Classification
Supports ResNet, VGG, AlexNet backbones for SOTA comparison
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple, Optional
import os

# å¯¼å…¥é€šç”¨ç»„ä»¶
from configs.universal_stage2_config import UniversalStage2Config
from models.resnet_stage2_classifier import ResNetStage2Loss  # å¤ç”¨æŸå¤±å‡½æ•°
from models.cnn_backbone import UniversalCNNBackbone


class UniversalStage2Classifier(nn.Module):
    """
    é€šç”¨Stage2åˆ†ç±»å™¨ï¼Œæ”¯æŒå¤šç§CNN backbone
    """

    def __init__(self, person_feature_dim: int, spatial_feature_dim: int,
                 hidden_dims: list = [256, 128, 64], dropout: float = 0.3,
                 fusion_strategy: str = "concat", backbone_name: str = "resnet18",
                 pretrained: bool = True, freeze_backbone: bool = False,
                 crop_size: int = 112):
        """
        Args:
            person_feature_dim: å•äººç‰¹å¾ç»´åº¦
            spatial_feature_dim: ç©ºé—´ç‰¹å¾ç»´åº¦
            hidden_dims: å…³ç³»ç½‘ç»œéšå±‚ç»´åº¦
            dropout: Dropoutæ¯”ç‡
            fusion_strategy: ç‰¹å¾èåˆç­–ç•¥
            backbone_name: Backboneåç§°
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            freeze_backbone: æ˜¯å¦å†»ç»“backbone
            crop_size: è£å‰ªå°ºå¯¸
        """
        super().__init__()

        self.backbone_name = backbone_name
        self.person_feature_dim = person_feature_dim
        self.spatial_feature_dim = spatial_feature_dim
        self.fusion_strategy = fusion_strategy

        # åˆ›å»ºCNN backboneç”¨äºäººå‘˜ç‰¹å¾æå–
        self.backbone = UniversalCNNBackbone(
            backbone_name=backbone_name,
            feature_dim=person_feature_dim,
            pretrained=pretrained,
            freeze_backbone=freeze_backbone,
            input_size=224  # æ ‡å‡†è¾“å…¥å°ºå¯¸
        )

        # ç©ºé—´ç‰¹å¾ç¼–ç å™¨ï¼ˆå¦‚æœæœ‰ç©ºé—´ç‰¹å¾ï¼‰
        if spatial_feature_dim > 0:
            self.spatial_encoder = nn.Sequential(
                nn.Linear(spatial_feature_dim, spatial_feature_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout)
            )
        else:
            self.spatial_encoder = None

        # è®¡ç®—å…³ç³»ç½‘ç»œè¾“å…¥ç»´åº¦
        relation_input_dim = self._get_relation_input_dim()

        # å…³ç³»ç½‘ç»œ
        relation_layers = []
        input_dim = relation_input_dim

        for hidden_dim in hidden_dims:
            relation_layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout)
            ])
            input_dim = hidden_dim

        # æœ€ç»ˆåˆ†ç±»å±‚
        relation_layers.append(nn.Linear(input_dim, 3))  # 3ä¸ªç±»åˆ«

        self.relation_network = nn.Sequential(*relation_layers)

        # Bilinearèåˆå±‚ï¼ˆå¦‚æœä½¿ç”¨bilinearç­–ç•¥ï¼‰
        if fusion_strategy == "bilinear":
            combined_dim = 2 * person_feature_dim + spatial_feature_dim if spatial_feature_dim > 0 else 2 * person_feature_dim
            self.bilinear = nn.Bilinear(combined_dim, combined_dim, hidden_dims[0])

        print(f"Created Universal Stage2 Classifier:")
        print(f"  Backbone: {backbone_name}")
        print(f"  Person feature dim: {person_feature_dim}")
        print(f"  Spatial feature dim: {spatial_feature_dim}")
        print(f"  Relation input dim: {relation_input_dim}")
        print(f"  Fusion strategy: {fusion_strategy}")

    def _get_relation_input_dim(self) -> int:
        """è®¡ç®—å…³ç³»ç½‘ç»œè¾“å…¥ç»´åº¦"""
        if self.fusion_strategy == "concat":
            return 2 * self.person_feature_dim + self.spatial_feature_dim
        elif self.fusion_strategy == "add":
            return self.person_feature_dim + self.spatial_feature_dim
        elif self.fusion_strategy == "bilinear":
            return self.hidden_dims[0]  # bilinearè¾“å‡ºåˆ°ç¬¬ä¸€ä¸ªéšå±‚
        else:
            raise ValueError(f"Unknown fusion strategy: {self.fusion_strategy}")

    def forward(self, person_A_images: torch.Tensor, person_B_images: torch.Tensor,
                spatial_features: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            person_A_images: [B, C, H, W] äººå‘˜Aå›¾åƒ
            person_B_images: [B, C, H, W] äººå‘˜Bå›¾åƒ
            spatial_features: [B, spatial_dim] ç©ºé—´ç‰¹å¾

        Returns:
            torch.Tensor: [B, 3] åˆ†ç±»logits
        """
        batch_size = person_A_images.size(0)

        # æå–äººå‘˜ç‰¹å¾
        person_A_features = self.backbone(person_A_images)  # [B, person_feature_dim]
        person_B_features = self.backbone(person_B_images)  # [B, person_feature_dim]

        # å¤„ç†ç©ºé—´ç‰¹å¾
        if self.spatial_encoder is not None and spatial_features.size(-1) > 0:
            spatial_features = self.spatial_encoder(spatial_features)  # [B, spatial_dim]
        else:
            spatial_features = torch.zeros(batch_size, 0).to(person_A_images.device)

        # ç‰¹å¾èåˆ
        if self.fusion_strategy == "concat":
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
            if spatial_features.size(-1) > 0:
                combined_features = torch.cat([
                    person_A_features, person_B_features, spatial_features
                ], dim=1)
            else:
                combined_features = torch.cat([
                    person_A_features, person_B_features
                ], dim=1)

        elif self.fusion_strategy == "add":
            # åŠ æ³•èåˆï¼ˆè¦æ±‚ç‰¹å¾ç»´åº¦ç›¸åŒï¼‰
            combined_features = person_A_features + person_B_features
            if spatial_features.size(-1) > 0:
                combined_features = torch.cat([combined_features, spatial_features], dim=1)

        elif self.fusion_strategy == "bilinear":
            # åŒçº¿æ€§èåˆ
            if spatial_features.size(-1) > 0:
                concat_features = torch.cat([
                    person_A_features, person_B_features, spatial_features
                ], dim=1)
            else:
                concat_features = torch.cat([
                    person_A_features, person_B_features
                ], dim=1)
            combined_features = self.bilinear(concat_features, concat_features)

        else:
            raise ValueError(f"Unknown fusion strategy: {self.fusion_strategy}")

        # å…³ç³»ç½‘ç»œæ¨ç†
        logits = self.relation_network(combined_features)  # [B, 3]

        return logits

    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        backbone_info = self.backbone.get_model_info()

        return {
            'model_type': 'universal_stage2_classifier',
            'backbone': self.backbone_name,
            'backbone_info': backbone_info,
            'person_feature_dim': self.person_feature_dim,
            'spatial_feature_dim': self.spatial_feature_dim,
            'fusion_strategy': self.fusion_strategy,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024
        }


def create_universal_stage2_model(config: UniversalStage2Config) -> nn.Module:
    """
    æ ¹æ®é…ç½®åˆ›å»ºé€šç”¨Stage2æ¨¡å‹

    Args:
        config: é€šç”¨Stage2é…ç½®å¯¹è±¡

    Returns:
        nn.Module: åˆ›å»ºçš„é€šç”¨Stage2æ¨¡å‹
    """
    # è·å–ç‰¹å¾ç»´åº¦
    person_feature_dim = config.get_person_feature_dim()
    spatial_feature_dim = config.get_spatial_feature_dim()

    # åˆ›å»ºæ¨¡å‹
    model = UniversalStage2Classifier(
        person_feature_dim=person_feature_dim,
        spatial_feature_dim=spatial_feature_dim,
        hidden_dims=config.relation_hidden_dims,
        dropout=config.dropout,
        fusion_strategy=config.fusion_strategy,
        backbone_name=config.backbone_name,
        pretrained=config.pretrained,
        freeze_backbone=config.freeze_backbone,
        crop_size=config.crop_size
    )

    print(f"âœ… Created Universal Stage2 Model:")
    print(f"   Backbone: {config.backbone_name}")
    print(f"   Person features: {person_feature_dim}D")
    print(f"   Spatial features: {spatial_feature_dim}D")
    print(f"   Fusion: {config.fusion_strategy}")

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    model_info = model.get_model_info()
    print(f"   Total parameters: {model_info['total_params']:,}")
    print(f"   Trainable parameters: {model_info['trainable_params']:,}")
    print(f"   Model size: {model_info['model_size_mb']:.1f} MB")

    return model


def create_universal_stage2_loss(config: UniversalStage2Config) -> ResNetStage2Loss:
    """
    åˆ›å»ºé€šç”¨Stage2æŸå¤±å‡½æ•°ï¼ˆå¤ç”¨ResNetçš„æŸå¤±å‡½æ•°ï¼‰

    Args:
        config: é€šç”¨Stage2é…ç½®å¯¹è±¡

    Returns:
        ResNetStage2Loss: æŸå¤±å‡½æ•°
    """
    if not hasattr(config, 'class_weights') or config.class_weights is None:
        config.class_weights = {0: 1.0, 1: 1.0, 2: 1.0}

    criterion = ResNetStage2Loss(class_weights=config.class_weights)
    print(f"âœ… Created Universal Stage2 Loss: weights={config.class_weights}")
    return criterion


def create_universal_optimizer(model: nn.Module, config: UniversalStage2Config) -> optim.Optimizer:
    """
    åˆ›å»ºé€šç”¨æ¨¡å‹çš„ä¼˜åŒ–å™¨

    Args:
        model: é€šç”¨æ¨¡å‹
        config: é…ç½®å¯¹è±¡

    Returns:
        torch.optim.Optimizer: ä¼˜åŒ–å™¨
    """
    # åˆ†ç¦»backboneå‚æ•°å’Œå…¶ä»–å‚æ•°ï¼Œä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
    backbone_params = []
    other_params = []

    model_for_iter = model.module if hasattr(model, 'module') else model
    if hasattr(model_for_iter, 'backbone'):
        for param in model_for_iter.backbone.parameters():
            if param.requires_grad:
                backbone_params.append(param)

        # å…¶ä»–å‚æ•°
        backbone_param_ids = {id(p) for p in backbone_params}
        for param in model_for_iter.parameters():
            if id(param) not in backbone_param_ids and param.requires_grad:
                other_params.append(param)
    else:
        # å¦‚æœæ²¡æœ‰backboneå±æ€§ï¼Œæ‰€æœ‰å‚æ•°ä½¿ç”¨ç›¸åŒå­¦ä¹ ç‡
        other_params = list(model.parameters())

    # è®¾ç½®å‚æ•°ç»„
    param_groups = []

    if backbone_params:
        backbone_lr = config.learning_rate * 0.1  # Backboneä½¿ç”¨1/10å­¦ä¹ ç‡
        param_groups.append({
            'params': backbone_params,
            'lr': backbone_lr,
            'name': 'backbone'
        })
        print(f"   Backbone params: {sum(p.numel() for p in backbone_params):,}, lr={backbone_lr}")

    if other_params:
        param_groups.append({
            'params': other_params,
            'lr': config.learning_rate,
            'name': 'classifier'
        })
        print(f"   Classifier params: {sum(p.numel() for p in other_params):,}, lr={config.learning_rate}")

    # åˆ›å»ºä¼˜åŒ–å™¨
    if config.optimizer == 'adam':
        optimizer = optim.Adam(
            param_groups if param_groups else model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
    elif config.optimizer == 'sgd':
        optimizer = optim.SGD(
            param_groups if param_groups else model.parameters(),
            lr=config.learning_rate,
            momentum=0.9,
            weight_decay=config.weight_decay
        )
    elif config.optimizer == 'adamw':
        optimizer = optim.AdamW(
            param_groups if param_groups else model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
    else:
        raise ValueError(f"Unknown optimizer: {config.optimizer}")

    print(f"âœ… Created {config.optimizer} optimizer with differential learning rates")
    return optimizer


def create_universal_training_setup(config: UniversalStage2Config, device: torch.device) -> Tuple:
    """
    åˆ›å»ºå®Œæ•´çš„é€šç”¨è®­ç»ƒè®¾ç½®

    Args:
        config: é…ç½®å¯¹è±¡
        device: è®¾å¤‡

    Returns:
        Tuple: (model, criterion, optimizer, scheduler)
    """
    print(f"\nğŸ—ï¸ Creating Universal training setup for {config.backbone_name}...")

    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    model = create_universal_stage2_model(config).to(device)

    # å¦‚æœæœ‰å¤šä¸ªGPUï¼Œä½¿ç”¨DataParallel
    if torch.cuda.device_count() > 1 and device.type == 'cuda':
        model = nn.DataParallel(model)
        print(f"   Using DataParallel on {torch.cuda.device_count()} GPUs")

    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = create_universal_stage2_loss(config).to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_universal_optimizer(model, config)

    # åˆ›å»ºè°ƒåº¦å™¨ï¼ˆå¤ç”¨ResNetçš„è°ƒåº¦å™¨åˆ›å»ºé€»è¾‘ï¼‰
    from utils.resnet_model_factory import create_resnet_scheduler
    scheduler = create_resnet_scheduler(optimizer, config)

    print(f"âœ… Universal training setup completed on {device}")

    return model, criterion, optimizer, scheduler


if __name__ == '__main__':
    # æµ‹è¯•é€šç”¨æ¨¡å‹å·¥å‚
    print("Testing Universal Model Factory...")

    from configs.universal_stage2_config import create_backbone_config

    # æµ‹è¯•ä¸åŒbackbone
    backbones = ['resnet18', 'vgg16', 'alexnet']

    for backbone in backbones:
        print(f"\n{'='*50}")
        print(f"Testing {backbone}:")

        # åˆ›å»ºé…ç½®
        config = create_backbone_config(
            backbone,
            visual_feature_dim=256,
            relation_hidden_dims=[256, 128, 64],
            dropout=0.3,
            batch_size=4
        )

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆ›å»ºè®­ç»ƒè®¾ç½®
        model, criterion, optimizer, scheduler = create_universal_training_setup(config, device)

        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 2
        person_A_images = torch.randn(batch_size, 3, 224, 224).to(device)
        person_B_images = torch.randn(batch_size, 3, 224, 224).to(device)
        spatial_features = torch.randn(batch_size, config.get_spatial_feature_dim()).to(device)
        targets = torch.randint(0, 3, (batch_size,)).to(device)

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            logits = model(person_A_images, person_B_images, spatial_features)
            loss, loss_dict = criterion(logits, targets)

        print(f"Input shapes:")
        print(f"  Person A/B: {person_A_images.shape}")
        print(f"  Spatial: {spatial_features.shape}")
        print(f"Output:")
        print(f"  Logits: {logits.shape}")
        print(f"  Loss: {loss.item():.4f}")

        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info() if hasattr(model, 'get_model_info') else \
                    model.module.get_model_info()
        print(f"Model info: {model_info}")

    print("\nâœ… Universal model factory test completed!")